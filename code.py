import streamlit as st
import os
from groq import Groq
import random

from langchain.chains import ConversationChain, LLMChain
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.messages import SystemMessage
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate


def main():
    """
    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.
    """
    base="light"
    # Get Groq API key
    groq_api_key = "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"
    # Display the Groq logo
    spacer, col = st.columns([5, 1])
    # The title and greeting message of the Streamlit application
    st.title("Code Generating AI")
    st.write("_What will you generate today?_")

    system_prompt = "be a code generating ai only"
    model = 'llama3-70b-8192'

    conversational_memory_length = 10

    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)

    messages = st.container(height=300)

    user_question = st.text_input("Generate Code!")

    messages.chat_message("user").write(user_question)

    generate = st.button("Generate", type="primary")

    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("Our story generating AI is based off of the 70 billion parameter Llama 3 model from Meta. It also uses up to 8192 tokens for max speed and efficiency. To give an idea how fast that it is, our model can generate 10 paragraphs of writing under a second! That is incredibly fast and we can assure you that this is the most frustrating-free experience you'll get - for free! That's right, no ads, no popups, nada! Try it out for yourself!")

    # session state variable
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history=[]
    else:
        for message in st.session_state.chat_history:
            memory.save_context(
                {'input':message['human']},
                {'output':message['AI']}
                )


    # Initialize Groq Langchain chat object and conversation
    groq_chat = ChatGroq(
            groq_api_key=groq_api_key,
            model_name=model
    )


    # If the user has asked a question,
    if user_question or generate:

        # Construct a chat prompt template using various components
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=system_prompt
                ),  # This is the persistent system prompt that is always included at the start of the chat.

                MessagesPlaceholder(
                    variable_name="chat_history"
                ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.

                HumanMessagePromptTemplate.from_template(
                    "{human_input}"
                ),  # This template is where the user's current input will be injected into the prompt.
            ]
        )

        # Create a conversation chain using the LangChain LLM (Language Learning Model)
        conversation = LLMChain(
            llm=groq_chat,  # The Groq LangChain chat object initialized earlier.
            prompt=prompt,  # The constructed prompt template.
            verbose=True,   # Enables verbose output, which can be useful for debugging.
            memory=memory,  # The conversational memory object that stores and manages the conversation history.
        )

        # The chatbot's answer is generated by sending the full prompt to the Groq API.
        response = conversation.predict(human_input=user_question)
        message = {'human':user_question,'AI':response}
        st.session_state.chat_history.append(message)
        messages.chat_message("assistant").write(response)

if __name__ == "__main__":
    main()




